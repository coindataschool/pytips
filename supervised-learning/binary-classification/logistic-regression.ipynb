{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "This notebook is a cheat sheet of logistic regression, which is one of the most commonly used machine learning models for solving binary classification problems. \n",
    "\n",
    "## Outline\n",
    "\n",
    "- [1 - Model Form, Loss Function, Gradient Descent](#1)\n",
    "- [2 - Logistic Regression in Scikit-learn](#2)\n",
    "- [3 - Logistic regression with regularization](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## Model Form, Loss Function, and Gradient Descent for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Form (Sigmoid Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have $n$ training examples and $p$ features. Let $X$ and $\\vec{y}$ \n",
    "denote the feature matrix and target respectively, we can then represent the \n",
    "training data as\n",
    "\n",
    "$$X = \n",
    "\\left(\\begin{array}{cc} \n",
    "x_{00}, x_{01}, x_{02}, ..., x_{0p}\\\\\n",
    "x_{10}, x_{11}, x_{12}, ..., x_{1p}\\\\\n",
    "\\vdots\\\\ \n",
    "x_{i0}, x_{i1}, x_{i2}, ..., x_{ip}\\\\\n",
    "\\vdots\\\\ \n",
    "x_{n0}, x_{n1}, x_{n2}, ..., x_{np}\n",
    "\\end{array}\\right),\n",
    "\n",
    "\\vec{y} = \n",
    "\\left(\\begin{array}{cc} \n",
    "y_{0}\\\\\n",
    "y_{1}\\\\\n",
    "\\vdots\\\\ \n",
    "y_{i}\\\\\n",
    "\\vdots\\\\ \n",
    "y_{n}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "Let $\\vec{x}_i = (x_{i0}, x_{i1}, ..., x_{ip})$, a row vector, denote the $i$-th training example.\n",
    "\n",
    "Let $\\vec{w} = (w_{0}, x_{1}, ..., x_{p})$, a row vector, denote the weights (or parameters) for each feature.\n",
    "\n",
    "We can then take their dot product to get a linear combination of the features for the $i$-th training example:\n",
    "\n",
    "$\\vec{w} \\cdot \\vec{x}_i = w_{0}*x_{i0} + w_{1}*x_{i1} + \\cdots + w_{p}*x_{ip}$.\n",
    "\n",
    "Geometrically, this equation is a line/plane/hyperplane passing through the origin (setting all $x$'s values to zero gives zero). To make it not pass the origin, let's add an intercept term $b$ and get a general linear function:\n",
    "\n",
    "$\\vec{w} \\cdot \\vec{x}_i + b = w_{0}*x_{i0} + w_{1}*x_{i1} + \\cdots + w_{p}*x_{ip} + b$.\n",
    "\n",
    "Let's demonstrate this in code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (5, 3)\n",
      "Target vector shape: (5,)\n",
      "Weight vector shape: (3,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# create a 5x3 feature matrix of random numbers: 5 examples and 3 features, and\n",
    "# a target of zeros and ones \n",
    "X = np.random.rand(5, 3)          # 2D numpy array\n",
    "y = np.array([1, 0, 0, 1, 1])     # 1D numpy array\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Target vector shape:\", y.shape)\n",
    "\n",
    "# initialize the weight vector and the constant\n",
    "w = np.array([0.1, 0.2, -0.5])    # 1D numpy array\n",
    "b = 3.5\n",
    "print(\"Weight vector shape:\", w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.13840009773898304"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the dot product of the weight vector and the feature vector of the \n",
    "# first example (X[0]), because w and X[0] are 1D arrays, we can use the numpy\n",
    "# function np.dot()\n",
    "np.dot(w, X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.361599902261017"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the constant \n",
    "np.dot(w, X[0]) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready for the model form of logistic regression. Recall that we want to predict a binary target, for example, \"yes\" or \"no\", \"good\" or \"bad\", \"late payment\" or \"on-time payment\", 0 or 1, -1 or 1, and etc. The linear function (of $\\vec{w}$ and $b$) above outputs real numbers, and if we can convert them to numbers between 0 and 1, we can interpret them as probabilities and predict positive label if we have a big probability and negative label if we have a small probability. So how do we convert real numbers to numbers between 0 and 1? We use the sigmoid function, $g(z) = \\frac{1}{1+e^{-z}}$. If we plug in $z_i = \\vec{w} \\cdot \\vec{x}_i + b$, we get the logistic regression model form $$\\hat{y}_i = \\frac{1}{1+e^{-(\\vec{w} \\cdot \\vec{x}_i + b)}}$$, where $i=1, ..., n$ is the $i$-th example. \n",
    "\n",
    "<center>  <img  src=\"./images/sigmoid-function.png\" width=\"600\" />  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code up the sigmoid function and apply it to the linear function output from the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9664826425608657"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(w, X[0]) + b) # we interpret the output of sigmoid function as probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function (Binary Cross Entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The loss for the $i$-th example (under logistic regression) is\n",
    "\n",
    "$-y_i*log(\\hat{y}_i)-(1-y_i)*log(1-\\hat{y}_i)$, where $y_i = 0, 1$ and $\\hat{y}_i=\\frac{1}{1+e^{-(\\vec{w} \\cdot \\vec{x}_i + b)}}$.\n",
    "\n",
    "When $y_i$ is 0, the loss simplifies to $-log(1-\\hat{y}_i)$. If $\\hat{y}_i \\approx 0$, the loss will be tiny; if $\\hat{y}_i \\approx 1$, the loss will blow up.\n",
    "\n",
    "When $y_i$ is 1, the loss simplifies to $-log(\\hat{y}_i)$. If $\\hat{y}_i \\approx 1$, the loss will be tiny; if $\\hat{y}_i \\approx 0$, the loss will blow up.\n",
    "\n",
    "By averaging over losses for all $i = 1, 2, ..., n$ examples, we get the overall loss function (also called binary cross entropy):\n",
    "\n",
    "$$ L(\\vec{w}, b) = \\frac{1}{n} \\sum_{i=1}^{n} -y_i*log(\\hat{y}_i)-(1-y_i)*log(1-\\hat{y}_i) $$\n",
    "\n",
    "Notice the loss is a function of $\\vec{w}$ and $b$. It is derived via MLE or MAP. Watch this excellent [lecture](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote06.html) for details. One thing to note, in the lecture, $\\vec{w}$ and $\\vec{x}_i$ are column vectors, so $\\vec{w}^{T}$ is a row vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w as a column vector has shape (3, 1)\n",
      "w transpose as a row vector has shape (1, 3)\n",
      "x0 as a column vector has shape (3, 1)\n",
      "The dot product of the weights and the 1st example's features values is -0.13840009773898304\n"
     ]
    }
   ],
   "source": [
    "# let's redo the dot product calculation using column vectors to convince \n",
    "# ourself the results are the same regardless how we represent the vectors\n",
    "w_col_vec = w.reshape(-1, 1)\n",
    "x0_col_vec = X[0].reshape(-1, 1)\n",
    "print(\"w as a column vector has shape\", w_col_vec.shape)\n",
    "print(\"w transpose as a row vector has shape\", w_col_vec.T.shape)\n",
    "print(\"x0 as a column vector has shape\", x0_col_vec.shape)\n",
    "print(\"The dot product of the weights and the 1st example's features values is\", np.dot(w_col_vec.T, x0_col_vec)[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product of the weights and the 1st example's features values, calculated with `np.matmul()`: -0.13840009773898304\n"
     ]
    }
   ],
   "source": [
    "# notice that in the above dot product calculation, we passed a 1x3 matrix (row vector)\n",
    "# and a 3x1 matrix (column vector) in the function np.dot(). Instead of using np.dot(), \n",
    "# we can view it as matrix multiplication and using np.matmul().\n",
    "print(\"The dot product of the weights and the 1st example's features values, calculated with `np.matmul()`:\", np.matmul(w_col_vec.T, x0_col_vec)[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature matrix has shape (5, 3)\n",
      "The weight vector has shape (3,)\n",
      "The product Xw has shape (5,)\n",
      "The values of Xw are [-0.1384001   0.01307232 -0.12151392 -0.41003077  0.0347996 ]\n"
     ]
    }
   ],
   "source": [
    "# The advantange of np.matmul() is that through vectorization, it allows fast \n",
    "# computation of dot products between the weight vector and the feature vectors \n",
    "# across all examples. This comes handy when you have thousands or millions of examples.\n",
    "print(\"The feature matrix has shape\", X.shape)\n",
    "print(\"The weight vector has shape\", w.shape)\n",
    "Xw = np.matmul(X, w) \n",
    "print(\"The product Xw has shape\", Xw.shape)\n",
    "print(\"The values of Xw are\", Xw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now code up the loss function for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_logistic(X, y, w, b):\n",
    "    \"\"\"Computes the loss for logistic regression via vectorization. Fast.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray (n,p)): feature matrix\n",
    "        y (ndarray (n,)) : target values (0 or 1)\n",
    "        w (ndarray (p,)) : feature weights\n",
    "        b (scalar)       : intercept\n",
    "    \"\"\"\n",
    "    z = np.matmul(X, w) + b \n",
    "    yhat = sigmoid(z)\n",
    "    loss = -np.sum(y * np.log(yhat) + (1-y) * np.log(1-yhat)) / len(y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.412359281839065"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_loss_logistic(X, y, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients (partial derivatives) with respect to the model parameters for logistic regression are\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w_j}} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i-y_i)x_{ij}$$ \n",
    "$$\\frac{\\partial{L}}{\\partial{b}} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i-y_i)$$\n",
    "\n",
    "where $\\hat{y}_i = \\frac{1}{1+e^{-(\\vec{w} \\cdot \\vec{x}_i + b)}}$, $i=1, ..., n$ is the $i$-th example, and $j=1, ..., p$ is the $j$-th feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code up a function to compute the gradients for logistic regression, given $X$, $\\vec{y}$, $\\vec{w}$, and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient_logistic(X, y, w, b):\n",
    "    \"\"\"Computes the gradients for logistic regression via vectorization. Fast.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray (n,p)): feature matrix\n",
    "        y (ndarray (n,)) : target values (0 or 1)\n",
    "        w (ndarray (p,)) : feature weights\n",
    "        b (scalar)       : intercept\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    z = np.matmul(X, w) + b\n",
    "    yhat = sigmoid(z)\n",
    "    gradient_w = np.matmul(X.T, yhat - y) / n\n",
    "    gradient_b = np.mean(yhat - y)\n",
    "    return gradient_w, gradient_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient w.r.t. w: [0.11410785 0.19006775 0.13217454]\n",
      "Gradient w.r.t. b: 0.3665408794153921\n"
     ]
    }
   ],
   "source": [
    "gradient_w, gradient_b = calc_gradient_logistic(X, y, w, b)\n",
    "print(\"Gradient w.r.t. w:\", gradient_w)\n",
    "print(\"Gradient w.r.t. b:\", gradient_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient_logistic_slow(X, y, w, b):\n",
    "    \"\"\"Computes the loss for logistic regression via for-loops. Slow.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray (n,p)): feature matrix\n",
    "        y (ndarray (n,)) : target values (0 or 1)\n",
    "        w (ndarray (p,)) : feature weights\n",
    "        b (scalar)       : intercept\n",
    "    \"\"\"    \n",
    "    m, n = X.shape\n",
    "    dL_dw = np.zeros(n)\n",
    "    dL_db = 0\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            z_i = np.dot(X[i], w) + b\n",
    "            yhat_i = sigmoid(z_i)\n",
    "            dL_dw[j] += (yhat_i - y[i]) * X[i,j]\n",
    "        dL_db += (yhat_i - y[i])\n",
    "    dL_dw /= m\n",
    "    dL_db /= m\n",
    "    return dL_dw, dL_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient w.r.t. w: [0.11410785 0.19006775 0.13217454]\n",
      "Gradient w.r.t. b: 0.3665408794153921\n"
     ]
    }
   ],
   "source": [
    "gradient_w, gradient_b = calc_gradient_logistic_slow(X, y, w, b)\n",
    "print(\"Gradient w.r.t. w:\", gradient_w)\n",
    "print(\"Gradient w.r.t. b:\", gradient_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a way to calculate the gradients, we can keep iterating and at each iteration, use the **entire** training examples, **simultaneously** update the model parameters according to the following equations:\n",
    "\n",
    "$$ w_j = w_j - \\alpha * \\frac{\\partial{L}}{\\partial{w_j}}, \\text{ for } j = 1, ..., p$$\n",
    "\n",
    "$$ b = b - \\alpha * \\frac{\\partial{L}}{\\partial{b}}$$\n",
    "\n",
    "We can stop when the model parameters stop changing much. This is the batch gradient descent algorithm, where 'batch' means using the entire training dataset at each iteration, and $\\alpha$ is the learning rate that we need to choose. If $\\alpha$ is tiny, the algorithm will take baby steps and slow to converge. If $\\alpha$ is big, the algorithm will take big steps and can easily jump around the local minima and never converge. By graphing the loss over iterations, we can see if we picked a good choice for $\\alpha$. If the loss keeps decreasing at reasonable speed, our learning rate works well. If the loss decreases very gradually by little amount over many iterations, our learning rate is too small. If the loss decreases and increases and oscillates, our learning rate is too big. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## How to train a logistic regression in Scikit-Learn and make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a dataset of 2306 [Uniswap V3 LP](https://twitter.com/coindataschool/status/1658997630930944000) positions that I downloaded from [Revert Finance](https://revert.finance/#/top-positions/arbitrum) on 27 Feb 2023. I've filtered and cleaned the data so that it contains equal number of positive (1) and negative (0) labels:\n",
    "\n",
    "- `is_profitable`: 0 or 1, where 1 means PnL > 0. This is the binary target we want our logistic regression to predict. \n",
    "\n",
    "and the following features:\n",
    "- `rng`: the difference between the upper and lower price limits of the LP position. It's a continuous feature that we can use to train a logistic regression.\n",
    "- `age`: number of days since position opening. A continuous feature.\n",
    "- `deposits_value`: dollar value of the deposits at position opening. A continuous feature.\n",
    "\n",
    "Keep in mind the goal here is NOT to train a model that predicts well, but rather to demonstrate the code for training a logistic regression in scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2306 entries, 161248 to 337749\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   is_profitable   2306 non-null   int64  \n",
      " 1   rng             2306 non-null   float64\n",
      " 2   age             2306 non-null   float64\n",
      " 3   deposits_value  2306 non-null   float64\n",
      "dtypes: float64(3), int64(1)\n",
      "memory usage: 90.1 KB\n"
     ]
    }
   ],
   "source": [
    "# read univ3 LP metrics on arbitrum\n",
    "df = pd.read_pickle(\"../data/univ3_lps_arb.pkl\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target:\n",
      " 1    1153\n",
      "0    1153\n",
      "Name: is_profitable, dtype: int64 \n",
      "shape: (2306,) \n",
      "\n",
      "features:\n",
      "                 rng        age  deposits_value\n",
      "nft_id                                        \n",
      "161248     3.306579  12.414294     3509.477545\n",
      "120168   509.645056  46.111574      250.430761\n",
      "342315     4.318640   6.983322      255.209643\n",
      "326145     2.816710  11.517616     1605.993331\n",
      "172488  2608.396616  85.732662    11442.193082 \n",
      "shape: (2306, 3)\n"
     ]
    }
   ],
   "source": [
    "# let's separate the target and the features\n",
    "y = df['is_profitable']\n",
    "X = df.drop(columns='is_profitable')\n",
    "print('target:\\n', y.value_counts(), '\\nshape:', y.shape, '\\n')\n",
    "print('features:\\n', X.head(), '\\nshape:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice each feature is on a different scale. For example, `deposits_value` has large values, whereas `rng`, `age`, and `delta24h_fee_apr` have values that are much smaller. This will cause issues training a logistic regression. The solution is to normalize the features to put them on the same/similar scale. Let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak to Peak range by column in Raw        X:\n",
      "rng               3.401887e+50\n",
      "age               5.383748e+02\n",
      "deposits_value    7.298510e+06\n",
      "dtype: float64\n",
      "\n",
      "Peak to Peak range by column in Normalized X:\n",
      "[ 6.34735761  7.06871935 17.31835943]\n"
     ]
    }
   ],
   "source": [
    "# scale the features\n",
    "scaler = StandardScaler()\n",
    "Xnorm = scaler.fit_transform(X)\n",
    "print(f\"Peak to Peak range by column in Raw        X:\\n{np.ptp(X, axis=0)}\\n\")\n",
    "print(f\"Peak to Peak range by column in Normalized X:\\n{np.ptp(Xnorm, axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the features are normalized, and the normalized features are more or less on a similar scale, we can train a logistic regression in 2 lines of code using the awesome Scikit-learn package, and use the model to predict the labels of the training examples and calculate accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction on training set: [1 1 1 ... 1 1 1]\n",
      "Accuracy on training set: 0.5082393755420642\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(Xnorm, y)\n",
    "\n",
    "# predict on training data\n",
    "yhat = lr_model.predict(Xnorm)\n",
    "print(\"Prediction on training set:\", yhat)\n",
    "\n",
    "# calc training accuracy\n",
    "print(\"Accuracy on training set:\", lr_model.score(Xnorm, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A training accuracy of 50.8% is hardly impressive, suggesting our logistic regression is as good as flipping a coin and hence underfit the data. A simple way to address underfitting is to add more features (specifically, non-linear features). It'll give a more complex model, increasing the chance of capturing the relationships between the features and the target. Let's add the 2nd-order terms of the existing features, i.e., squared terms like $x_j^2$ and iteraction terms like $x_jx_k$ and retrain a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2306, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# add 2nd order terms \n",
    "Xpoly = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X)\n",
    "print(Xpoly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak to Peak range by column in Raw        X:\n",
      "[3.40188746e+050 5.38374768e+002 7.29850977e+006 1.15728383e+101\n",
      " 1.71194241e+053 4.75080881e+053 2.95251316e+005 3.84247810e+009\n",
      " 5.32689748e+013]\n",
      "\n",
      "Peak to Peak range by column in Normalized X:\n",
      "[ 6.34735761  7.06871935 17.31835943  6.36133299 16.55654053 26.86789077\n",
      "  8.99955996 18.0801493  19.07802454]\n"
     ]
    }
   ],
   "source": [
    "# scale the features\n",
    "scaler = StandardScaler()\n",
    "Xpoly_norm = scaler.fit_transform(Xpoly)\n",
    "print(f\"Peak to Peak range by column in Raw        X:\\n{np.ptp(Xpoly, axis=0)}\\n\")\n",
    "print(f\"Peak to Peak range by column in Normalized X:\\n{np.ptp(Xpoly_norm, axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction on training set: [0 1 0 ... 0 0 0]\n",
      "Accuracy on training set: 0.5407632263660017\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "poly_lr_model = LogisticRegression(max_iter=100000)\n",
    "poly_lr_model.fit(Xpoly_norm, y)\n",
    "\n",
    "# predict on training data\n",
    "yhat = poly_lr_model.predict(Xpoly_norm)\n",
    "print(\"Prediction on training set:\", yhat)\n",
    "\n",
    "# calc training accuracy\n",
    "print(\"Accuracy on training set:\", poly_lr_model.score(Xpoly_norm, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding 2nd-order terms bumps the training accuracy to 0.54. Lol. Let's output the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [ 0.04662739  0.56377792 -0.34371193  0.03398941 -0.40191915  0.12354558\n",
      " -0.79012122 -0.26386526  0.58270503]\n",
      "b: -0.03068733882615702\n"
     ]
    }
   ],
   "source": [
    "print(\"w:\", poly_lr_model.coef_[0])\n",
    "print(\"b:\", poly_lr_model.intercept_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "\n",
    "## Logistic Regression with Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more features can eventually lead to overfitting, i.e., the model fits too well to the training data, capturing both the signals and noise in the training data, and hence fails to generalize to new data. We can use regularization to reduce overfitting. \n",
    "\n",
    "The loss function for (L2) regularized logistic regression is\n",
    "\n",
    "$$ L(\\vec{w}, b) = \\frac{1}{n} \\sum_{i=1}^{n} -y_i*log(\\hat{y}_i)-(1-y_i)*log(1-\\hat{y}_i) + \\frac{\\lambda}{2n} \\sum_{j=1}^{p}{w_j^2}$$\n",
    "\n",
    "and $\\sum_{j=1}^{p}{w_j^2}$ is called the L2 regularization term.\n",
    "\n",
    "The gradients for (L2) regularized logistic regression are\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w_j}} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i-y_i)x_{ij} + \\frac{\\lambda}{n}w_j$$ \n",
    "$$\\frac{\\partial{L}}{\\partial{b}} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i-y_i)$$\n",
    "\n",
    "Remarks:\n",
    "\n",
    "- L2 (also called Ridge) means squaring the weights in the regularization term. If we take the absolute value instead, we have L1 (also called LASSO) regularization. \n",
    "- L2 regularization shrinks the weights toward zero but not equal to zero, whereas L1 regularization sets some weights exactly to zero. \n",
    "- L2 regularization is used more often. \n",
    "- $\\lambda$ is called the regularization strength. The larger the $\\lambda$, the smaller the weights.\n",
    "\n",
    "Let's now train a L2-regularized logistic regression on 9 features (both the 1st-order and 2nd-order terms) of the LP positions data above.  One thing to note, in scikit-learn's logistic regression, there is no $\\lambda$ parameter, instead, there's a parameter C, which is $\\frac{1}{\\lambda}$, so \n",
    "\n",
    "- Smaller C: Stronger regularization (more penalty on large coefficients, resulting smaller weights).\n",
    "- Larger C: Weaker regularization (less penalty on large coefficients, resulting bigger weights).\n",
    "\n",
    "Let's check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [ 0.05003916  0.58508146 -0.37478974  0.03106631 -0.41784212  0.13111822\n",
      " -0.81412158 -0.39189155  0.72538552]\n",
      "b: -0.03431939971138834\n"
     ]
    }
   ],
   "source": [
    "# train a L2 regularized logistic regression with C = 10\n",
    "regu_lr_model = LogisticRegression(C=10, penalty='l2')\n",
    "regu_lr_model.fit(Xpoly_norm, y)\n",
    "\n",
    "# check model parameters\n",
    "print(\"w:\", regu_lr_model.coef_[0])\n",
    "print(\"b:\", regu_lr_model.intercept_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [ 0.02623997  0.07879308 -0.06469286  0.02574982 -0.15312429  0.01232702\n",
      " -0.22341705 -0.03985615  0.03807066]\n",
      "b: -0.007199800511723452\n"
     ]
    }
   ],
   "source": [
    "# train a L2 regularized logistic regression with C = 0.01\n",
    "regu_lr_model = LogisticRegression(C=0.01, penalty='l2')\n",
    "regu_lr_model.fit(Xpoly_norm, y)\n",
    "\n",
    "# check model parameters\n",
    "print(\"w:\", regu_lr_model.coef_[0])\n",
    "print(\"b:\", regu_lr_model.intercept_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see `C=0.01` results smaller weights (stronger regularization) than `C=10`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we summarized the key knowledge points for logistic regression, including its model form, loss function, gradient function, gradient descent, and L2 regularization. We also demonstrated how to use numpy's dot product and matrix multiplication to implement logistic regression's loss function and gradient calculation via vectorization. Finally, we showed how to train a (regularized) logistic regression in scikit-learn. \n",
    "\n",
    "## Learning Resources\n",
    "\n",
    "- [Supervised Machine Learning: Regression and Classification](https://www.coursera.org/learn/machine-learning).\n",
    "- [Machine Learning for Intelligent Systems](https://www.cs.cornell.edu/courses/cs4780/2018fa/page18/).\n",
    "\n",
    "## Referral\n",
    "\n",
    "- Digital Ocean is a cloud computing platform where you can rent remote servers for cheap. \n",
    "  I have my remote data science server there. You can do the same and [get $200 credit](https://m.do.co/c/0a435cb96813). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
